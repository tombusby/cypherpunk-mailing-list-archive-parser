From: jim bell <jimbell@pacifier.com>
Date: Fri, 14 Jun 1996 10:39:37 +0800
To: cypherpunks@toad.com
Subject: Re: PBS show
Message-ID: <199606131831.LAA29180@mail.pacifier.com>
MIME-Version: 1.0
Content-Type: text/plain


At 10:31 AM 6/13/96 -0700, Timothy C. May wrote:

>
>Compared to my SOL, the Apple II would've been more useful, in retrospect.
>As to the 40-character display, that was the norm in those days. (My SOL,
>and certainly the other machines available to home users at that time, had
>only a 40-character-wide display...when in worked.)
>
>The comments about the floppies (" VERY SMALL capacity floppies (which were
>very slow as well") is even more off-base.

The original Apple II floppy held ONLY 90 kilobytes on a 5" floppy.  How did 
they do such a bad job?

>>BTW, Intel shares a substantial proportion of the blame for Apple's choice
>>of the 6502.  The decision was made, I've heard, because Intel was still
>>trying to get $200 for a slow 8080, while Western Design Center (?) wanted
>>only about $20 for a 6502.
>
>You need to check your claims more carefully. There are always many reasons
>a chip is selected for a design.

Yes, there are, but a 10-to-1 difference in price is a killer for most designs.

>
>>And by refusing to build Masatoshi (?) Shima's design for the Z-80, they
>>totally lost the race for the 8-bit PC world.  The Z-80 turned into the
>>highest-volume 8-bit microprocessor by far, leaving both the 8080 and the
>>8085 in the dust, and even the 6502.
>
>Again, your understanding of the history of Intel, Zilog, and the industry
>in general is lacking.
>
>The design and process technology resources were instead committed to the
>8086, 

The design for the Z-80 was completed and in Intel's hands.  Intel didn't 
want to build the Z-80, they wanted to focus on peripheral chips, so they 
let Shima go and start Zilog.


and history is rather clear about the wisdom of doing that. Intel is
>now capitalized at something like $50-70 billion, and Zilog is no longer on
>the radar screen.

No!  The 8086 wasn't "wise," it was a crock. (The _principle_ of building a 
16-bit microprocessor was just fine, and in fact necessary.  It's their 
product that sucked.)   Brain-dead segmented architecture, 64k per segment 
limitation, 1-megabyte memory limitation.  The only thing that made it 
appear like a wise move (and even then, only in hindsight) was the fact that 
IBM was even more idiotic, and selected it (actually, worse, the 8088!) for 
their PC.  Even a bad standard can be hugely profitable, and that is what 
kept Intel alive.

Furthermore, the only way Intel got the 8086 off the ground was to buy off 
AMD to support the 8086, as opposed to the Z-8000 as they had done in a long 
series of anti-8086 ads in about 1978 or so.  (you do recall the series, 
don't you?  You know, the two guys on the soapboxes?)  They later stiffed 
AMD by refusing to deliver the design for the 386, and they got raked over 
the coals for that a few years ago in court.  Pure fraud.  Intel's 
misbehavior has probably kept the price of the leading-edge generation 
microprocessor at least a factor of 3 higher than it would have been had AMD 
been allowed to compete according to their 1978 agreement.  Much of the 
capitalization you speak of, therefore, was the product of this fraud.

The world would have been far better off if they'd chosen the 68000 for the 
IBM PC.  I suspect the reason IBM didn't was that they thought it would be 
easier to push around tiny Intel rather than the electronics giant Motorola. 
 They were probably right about this limited interpretation, but in exchange 
for a little temporary control they doomed the world to a built-in series of 
design crocks that only began with the innate limitations of the 8088. 
 
Every product for the PC you buy today is more expensive, less reliable, 
harder to install, less compatible, more complicated for less benefit, has 
less competition, and is in general less desirable because of the mistakes 
Intel made in the middle 1970's, and IBM made in 1980 or so.  These mistakes 
weren't repaired in subsequent incarnations of the 286, 386 and others, or 
IBM's AT, etc, because they were _architecture_ problems and software-design 
problems that cannot be "repaired."  

Why is it that you can afford 32 megabytes of DRAM, yet you'd get 
insufficient-memory errors if you're not careful with you CONFIG.SYS and 
AUTOEXEC.BAT?  (Combination of Intel's dumb mistake of a 1-meg memory 
limitation, and IBM's filling of 1/3 of that space with crap.)  Why is it 
that you can't put two color displays on the latest PC's? (IBM's dumb 
mistake:  Memory-mapped video that can't be re-addressed.) Why can't you put 
a dozen peripheral devices into a PC, and you have trouble with even 3-4, 
and are forced to look up all the DMA's, Interrupts, and COM ports to ensure 
no overlap?  (IBM's dumb mistake.)

I could go on, but what's the point?  The PC architecture is pure crap, and 
I'm saying that as a person who uses them daily, and will not own a Mac 
because of Apple's legal misbehavior.


Jim Bell
jimbell@pacifier.com





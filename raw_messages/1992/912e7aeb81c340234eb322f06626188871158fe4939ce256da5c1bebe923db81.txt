From: fnerd@smds.com (FutureNerd Steve Witham)
Date: Mon, 7 Dec 92 08:26:16 PST
To: cypherpunks@toad.com
Subject: CHATTER Re: Broadcast/Filter vs. "Fetching"
Message-ID: <9212071546.AB14663@smds.com>
MIME-Version: 1.0
Content-Type: text/plain


(I put "CHATTER" in the subject line as a clue that this is general talk
off the subject of directly implementing crypto stuff.)

Yanek Martinson    <yanek@novavax.nova.edu> sent an interesting message
to me--I think he meant to post it--in response to my gripe:

>> (I mean, while I'm at it, I hate the broadcast-and-filter model of netnews,
>> CD ROMs, and cable TV.  I want things available for me to go fetch.)
>
>But if you have the disk space and bandwidth to spare, it really does
>make sense to receive everything and then decide what you want.

Perhaps our company could make room for a couple days worth of the complete
netnews.  But,
  o  That's hardly all the information there is.
  o  I could never read it all, and I don't trust computer filters much.
  o  I don't want to be a slave to the newsfeed/use-it-or-lose-it deal;
     I want to follow threads into the past, at my leisure.  I want to let
     some discussions cook and then come back to them.

>In your "fetch" scenario, what if I don't like (strongly) what Joe wrote.
>So I can nuke Joe (or his archive) and no-one can "Fetch" the article.

This is what I found interesting.  You would really like the network (some
network) to support distributed redundant archives, preferably with cross-
checking.  It would be even nicer if information could be stored, with
storage cost charged to the owner and/or readers, and yet not have it 
knowable where the information was.  Somehow a backwards mix setup.  Put 
on your crypto thinking caps, kids.

>On the other hand with the current set-up, it has already propagated 
>to hundreds of thousands of sites and it's impossible to stop.

Yes.  But, say, tens of untraceable sites would be okay.  The flip side of
100K x redundancy is the little emily postnews handslap--"ARE YOU SURE?"--
and the rigamarole over creating new groups, and whether all the machines
between you and the center of the universe get all the groups, etc.
Plus the fact that most sites delete the stuff after a week or two, and
all the effects on people's writing that has.

>Also, if you want to filter stuff, then you should pay for the CPU
>that does the filtering for you.

My brain filters my mail!  Boy do I pay for it!

>Why would you expect someone else
>to run your filter on their machine just so you would know if you
>wanted to fetch their article.

??  I don't want machine filtering at all.  Mostly I want references I can
follow up easily.  That's what fetching means.

>Maybe I did not understand what you meant by "fetching".  

Hope it's clearer.

>Could you describe your idea in more detail?
>I absolutely agree that most of what comes in over my
>newsfeed is junk, I just don't see a good way of 
>fixing this problem while retaining the robustness
>and widespread communications capability.
>
>> -fnerd
>> quote me...within *reason* of course
>
>(I guess 4 lines (including the sig) is well within reason)

-fnerd
quote me
fnerd@smds.com (FutureNerd Steve Witham)





